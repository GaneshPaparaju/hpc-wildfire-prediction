# ğŸ”¥ HPC Wildfire Ignition Prediction

A High-Performance Computing (HPC) project demonstrating **parallel feature engineering, scalable model training, and performance analysis** on large-scale spatiotemporal wildfire data.

This project focuses on **when parallelization works, when it doesnâ€™t, and why** â€” using real measurements instead of toy examples.

---

## ğŸ“Œ Project Overview

Wildfire ignition prediction requires processing massive temporal weather datasets.  
This project applies **HPC techniques** to accelerate a full machine-learning pipeline built on:

- **9.5 million daily meteorological observations**
- **37,000+ spatial locations across the US**
- **12 years of data (2013â€“2025)**

The primary goal is **computational performance analysis**, not just predictive accuracy.

---

## ğŸ¯ Objectives

- Parallelize **60-day rolling feature engineering**
- Benchmark **Joblib, Dask, and built-in multithreading**
- Measure **speedup, efficiency, and overhead**
- Validate **Amdahlâ€™s Law** empirically
- Compare **XGBoost vs Random Forest** from an HPC perspective

---

## ğŸ§  Key HPC Concepts Demonstrated

- Task granularity and load balancing
- CPU core scaling and saturation
- Speedup vs efficiency trade-offs
- Parallel overhead analysis
- Frameworkâ€“algorithm compatibility

---

## ğŸ—ï¸ Pipeline Summary

1. Parallel CSV loading (Dask)
2. Data preprocessing and balancing
3. 60-day rolling feature engineering (Joblib)
4. Hyperparameter search (Joblib + scikit-learn)
5. Model training (XGBoost & Random Forest)
6. HPC benchmarking across multiple CPU counts

---

## âš™ï¸ Technologies Used

- **Python** (Pandas, NumPy, Scikit-learn)
- **Joblib** (process-based parallelism)
- **Dask** (distributed execution)
- **XGBoost**
- **Random Forest**
- **Slurm-based HPC cluster (Explorer OOD)**

---

## ğŸ“Š Parallel Feature Engineering Results

### Rolling Feature Engineering (Optimized Joblib)

| CPU Cores | Time (s) | Speedup | Efficiency |
|----------|----------|---------|------------|
| 1        | 417.3    | 1.00Ã—   | 100% |
| 8        | 150.8    | 2.77Ã—   | 34.6% |
| 16       | 93.1     | 4.48Ã—   | 28.0% |
| 32       | 59.3     | 7.04Ã—   | 22.0% |
| 50       | 42.9     | **9.72Ã—** | **19.4%** |

**Key Insight:**  
Initial naÃ¯ve parallelization achieved only **~1.1Ã— speedup**.  
After workload-balanced chunking, speedup improved to **9.72Ã—**.

---

## ğŸ“ Amdahlâ€™s Law Validation

- Parallel fraction: **89.9%**
- Sequential overhead: **10.1%**
- Theoretical maximum speedup: **9.9Ã—**
- Observed speedup: **9.72Ã—**

â¡ï¸ Achieved **~98% of the theoretical maximum**, clearly validating Amdahlâ€™s Law.

---

## ğŸ” Hyperparameter Search Scaling

| CPU Cores | Speedup | Efficiency |
|----------|---------|------------|
| 4        | 3.84Ã—   | 96% |
| 8        | 5.76Ã—   | 72% |
| 16       | 9.39Ã—   | 59% |

**Lesson:**  
Coarse-grained tasks (full model training runs) parallelize extremely well.

---

## ğŸ¤– Model Training: XGBoost vs Random Forest

### Best Observed Configurations

| Model | Parallel Method | CPUs | Wall Time (s) | Speedup | Efficiency |
|-----|----------------|------|---------------|--------|------------|
| XGBoost | Dask Distributed | 8 | **42.5** | 2.0Ã— | 25% |
| Random Forest | Built-in Threads | 28 | 210.2 | **16.4Ã—** | 59% |

### Interpretation

- **XGBoost**
  - Fastest absolute runtime
  - Limited scaling due to boostingâ€™s sequential nature
- **Random Forest**
  - Near-linear scaling
  - Excellent example of embarrassingly parallel workloads

---

## ğŸ“ˆ Visual Results

Key performance plots are available in the `/figures` directory:
- Rolling feature speedup & efficiency
- Hyperparameter search scaling
- XGBoost vs Random Forest scaling comparison

---

## ğŸ§ª Machine Learning Performance (Secondary Focus)

| Model | Accuracy | ROC-AUC |
|-----|---------|---------|
| XGBoost | ~0.62 | ~0.58 |
| Random Forest | ~0.66 | ~0.55 |

Predictive accuracy was intentionally treated as **secondary** to HPC analysis.

---

## ğŸ”‘ Key Takeaways

- More CPU cores do **not** guarantee better performance
- Task design matters more than framework choice
- Joblib excels for coarse-grained CPU tasks
- Dask introduces overhead on single-node workloads
- Built-in threading is often optimal for tree-based models

---

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ HPC_TEAM10.ipynb        # Main HPC experiments & benchmarks
â”‚   â””â”€â”€ HPC_TEAM10_EDA.ipynb    # Exploratory data analysis
â”œâ”€â”€ figures/                   # Saved benchmark plots
â”œâ”€â”€ report/
â”‚   â””â”€â”€ TEAM_10_Report_HPC.pdf  # Full technical report
â””â”€â”€ README.md
